% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{cmuthesis} % use larger type; default would be 10pt
\linespread{2}

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage[table,xcdraw]{xcolor}

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdflscape}
%\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{bibentry}

\usepackage{algorithm2e}
%\usepackage{algorithmicx}

\usepackage[english]{babel}

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
%\usepackage{booktabs} % for much better looking tables
%\usepackage{array} % for better arrays (eg matrices) in maths
%\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
%\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
%\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...





%%% END Article customizations

%%% The "real" document content comes below...

\title{Variational Methods for Energy Systems}
\author{Henning Lange}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}

\input{content/preconfig}
\input{content/frontmatter}
\input{content/postconfig}


\chapter{Introduction}

According to the Environmental Protection Agency (EPA), in 2014, carbon dioxide ($CO_2$) made up 81\% of all green house gases emitted in the United States~\cite{EPA}. Furthermore, it is reported that ``the combustion of fossil fuels to generate electricity was the largest single source of $CO_2$ emissions in the nation, accounting for about 35 percent of total U.S. $CO_2$ emissions and 29 percent of total U.S. greenhouse gas emissions"~\cite{EPA} making electricity generation the single largest contributor to green house gas emissions overall. See Figures \ref{fig:intro} and \ref{fig:intro2} for a breakdown of $CO_2$ emissions by source and the makeup of green house gas emissions.
\begin{figure}
\centering
\includegraphics[width=0.45\linewidth]{co2b.png}
\caption[Total share of green house gas emissions by sector.]{Total share of green house gas emissions by sector~\cite{EPA}.}
\label{fig:intro}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.50\linewidth]{ghgs.png}
    \caption[Make up of green house gases emitted]{Make up of green house gases emitted~\cite{EPA}.}
    \label{fig:intro2}
\end{figure}

Green house gas emissions in turn are the primary driver of climate change~\cite{change2014mitigation} and according to the EPA, climate change impacts society in many different ways. For example, climate change can influence rainfall and crop yields, affect human health, and impact forests and other ecosystems, as well as have adverse affects on the supply-side of the electrical grid."~\cite{EPA}.\\
Apart from these non-monetary costs, energy production also incurs substantial monetary expenditures: The Federal Energy Regulations Commission (FERC) estimates the total energy generation cost for the U.S. to be 112B USD annually~\cite{ferc2012history}, thus making minuscule improvements in energy efficiency often financially viable. However, considering the fact that renewable energy sources have become or are on the verge of becoming cheaper than fossil fuels~\cite{eia2016} and cost adverse consumers try to minimize energy consumption anyways, i.e. reducing energy consumption or increasing energy efficiency seems to be in everyones interest, it is pertinent to ask why sustainable energy sources have not yet found widespread adoption. The American Energy Innovation Council identifies two reasons: apart from politicism, they opine that technological advancements are required to reduce green house gas emissions associated with electricity generation and consumption~\cite{AEIC}.\\
In this work, two problems are identified and solutions are proposed whose adoption can lead to improvements to energy efficiency and energy conservation. They encompass inference and control problems on both the demand- and generation-side of the electrical grid and, as we will show later, the reason why these problems are challenging are due to computational issues, i.e. computing the exact solution to these problems is intractable because the computational cost oftentimes grows exponentially with the problem size. As we will show later, these problems share that the intractability stems from the difficulty of computing posterior distributions over binary configurations. The intractability of posterior distributions in turn usually stems from the difficulty of computing the Bayesian inversion:
\begin{align}
p(z|x) = \frac{p(x,z)}{\sum_{z' \in \mathcal{Z}} p(x,z')} \label{eq:bayes_inversion}
\end{align}
 where $z$ and $\mathcal{Z}$ denote the latent variable and its domain respectively and $x$ specifies some measured quantity. Note that for most latent domains of interest, computing the sum in the denominator (or integral when the latent variable is continuous) is computationally very hard. For the problems at hand, because the latent variable is multi-dimensional and binary, the computational complexity is in $\mathcal{O}(2^N)$ where $N$ denotes the dimensionality of the latent variable $z$. Furthermore, note that for many distributions that model temporal dependencies even computing the joint distribution can often be intractable~\cite{ghahramani1996factorial}. This intractability usually stems from the computational difficulty of computing the forward probabilities:
 \begin{align}
     p(x_t, z_t | x_{1:t-1}) = p(x_t|z_t) \sum_{z' \in \mathcal{Z}} p(z_t| z') p(z'|x_{1:t-1}) \label{eq:forward_probs_intro}
 \end{align}
 Note that similarly to (\ref{eq:bayes_inversion}), evaluating (\ref{eq:forward_probs_intro}) requires a summation over the latent domain $\mathcal{Z}$ which, again, can be computationally expensive if $\mathcal{Z}$ is large.\\
 
 When discussing these problems in more depth, we will show that current solutions make use of approximations and often-times greedy simplifying assumptions and heuristics in order to circumvent these computational costs. In this work, improvements to these approximations are sought. Specifically, we will make use of recent technological advancement in the field of Machine Learning for posterior inference that, in principle and under some conditions, allow for asymptotically exact solutions. As we will show later, we will make use of Variational Inference which circumvents the need to compute the Bayesian inversion in (\ref{eq:bayes_inversion}) by minimizing a divergence measure between the true posterior $p(z|x)$ and an approximate posterior.

\section{Problem descriptions}
We focus on two specific problems in this thesis whose solutions require obtaining an optimal binary vector, which in turn entails that $\mathcal{Z} = \{0,1\}^N$. These problems constitute Non-Intrusive Load Monitoring and Alternating Current Optimal Power Flow. The semantics of this optimal vector is different depending on the problem: In the case of Non-Intrusive Load Monitoring the optimal vector describes the most likely state of appliances in a building, whereas for Alternating Current Optimal Power Flow this optimal vector describes which generators are \emph{on} or \emph{off} in the most cost efficient configuration of generators. In the following section, brief descriptions of these problems.

\subsection{Demand-side sensing: Non-Intrusive Load Monitoring} 
Buildings account for 73\% of the energy and 40\% of the electricity consumption in the United States~\cite{li2014review}. However, knowledge about how buildings consume energy is scarce, i.e. end-users are typically faced with a monthly aggregate electricity bill. Even though, as discussed earlier, it is in the end-users interest to save energy, more fine-grained information about how much and when individual appliances consume electricity is often required to actually achieve this. According to a 2013 study~\cite{armel2013disaggregation}, providing feedback about the power consumption of individual appliances can lead to savings between 12-15\% of the energy consumed. On top of that, appliance-level energy consumption information can have secondary use-cases such as e.g. demand response~\cite{he2013incorporating}, geriatric care~\cite{alcala2015detecting}, fault detection~\cite{denucci2005diagnostic} and so on. However, obtaining this information can be costly, i.e. installing a single meter for individual appliances can be prohibitively expensive, because potential energy savings do not justify installation and maintenance costs of electricity meters for every appliance. Non-Intrusive Load Monitoring (NILM) could potentially alleviate this problem~\cite{hart1992nonintrusive}. For recent reviews of the technique, the reader is referred to ~\cite{zoha2012non,zeifman2011nonintrusive,faustine2017survey}. NILM, also called energy disaggregation, is a class of source separation algorithms whose goal it is to infer the energy consumption of individual appliances algorithmically given measurements collected at a limited number of sensing points in a building. Specifically, because power as well as current are additive, a small number of sensors is usually installed at the main distribution panel where the sum of the power draws of appliances is measured. NILM then tries to break down this sum into its summands. Arguably, one of the reasons why decades of NILM research has not produced acceptable solutions is in part due to the difficulty of the problem: Inferring the operational states of appliances is often cast as a posterior inference problem and the associated posterior distributions are usually intractable.

\subsection{Generation-side control: AC Optimal Power Flow}
Alternating current optimal power flow (ACOPF) is the scholarly term for the decades old problem of finding the optimal configuration of power generators such that demands are met throughout an alternating current transmission network, where optimality is usually defined in terms of generation cost. For a review of ACOPF and modern techniques to tackle the problem, the reader is referred to \cite{ferc2012history,capitanescu2016critical}. Note that suboptimal configurations can lead to unnecessary waste due to transmission losses and unnecessary costs due to not fully utilizing cheap generation sources. However, the potential payoff of improvements to existing solutions is big: The FERC estimates that the introduction of mixed-integer programming approaches has already saved over one-half billion dollars yearly and projects that a 5\% increase in optimality could save consumers another 6 billion dollars annually~\cite{ferc2012history}.\\
However, despite the size of potential payoffs, the FERC opines that algorithms that produce robust, fast and optimal solutions do not exist even decades after the inception of the problem~\cite{ferc2012history}. This is arguably also due to the complexity of the problem: When ACOPF is cast as a constrained optimization problem, the constraints pose computational challenges. Some of the constraints are non-linear whereas others are non-convex. As we will show later, compliance with the non-convex constraints can be achieved by turning the problem into a posterior inference problem, though, this posterior distribution is, again, intractable.



\section{Non-Intrusive Load Monitoring}

As described earlier, Non-Intrusive Load Monitoring is the problem of inferring the power consumption of individual appliances given measurements obtained at a limited number of sensing points, specifically often measurements collected at the main electrical feed of a building. The problem was first described in the seminal paper by George Hart in 1992~\cite{hart1992nonintrusive}. 

%The goal of NILM is to ultimately reduce the cost of providing appliance-level electricity measurements within a building. In the next section the potential benefits of NILM in different scenarios are discussed.
\iffalse
\section{When to use Non-Intrusive Load Monitoring?}

As mentioned earlier, the goal of NILM is to provide a service cheaper than a competing approach. In this case, the competing approach is Intrusive Load Monitoring (ILM), i.e. installing a sensor for every single appliance. Because NILM algorithms provide appliance state \emph{estimates} as opposed to real measurements, i.e. the value that NILM provides is arguably smaller compared to ILM, the question arises in which scenarios NILM is the appropriate choice for appliance load monitoring.\\
The answer to this question is dependent on numerous factors:
\begin{description}
\item[Load Monitoring application] Load monitoring in itself does provide little value without a specific intention on what to use the additional information for. Just providing the information of appliance level electricity consumption to e.g. residential customers in some cases even lead to an increase in consumption~\cite{kelly2016does}. However, there are tasks for which appliance-level consumption information is helpful such as:
\begin{itemize}
    \item Demand response: If a specific reduction in peak demand is to be achieved, appliance-level electricity consumption can facilitate the identification of appropriate actions to meet the reduction goal. For example, the knowledge that a certain relaxation of the temperature set point of an HVAC system reduces its energy consumption by a certain amount requires knowledge of the power consumption of the HVAC system.~\cite{he2013incorporating}
    \item Fault detection: Appliance faults can potentially be identified by the means of load monitoring. For example, faulty gaskets or sealings will increase the electricity consumption of fridges. Such an increase could potentially be detected by the means of load monitoring. The value of load monitoring for fault detection is a potential reduction in energy waste and improved lifetime of appliances.
    \item Operational point verification: Opposed to inferring whether or not appliances are faulty, load monitoring could also be used to verify if appliances are used correctly. Load monitoring could be used to verify if e.g. lighting are turned off at night, etc.
    \item Activity detection: Appliance usage patterns can, in principle, be used to infer information about occupants. Load monitoring could be used for occupancy detection or, going one step further, to infer 'Activities of Daily Living' (ADL). Inferring ADLs in a cost effective way could be helpful in e.g. geriatric care and reduce the need for either additional sensing equipment or other intrusive or costly methods to infer ADLs such as interviews.~\cite{alcala2015detecting}
\end{itemize}
\item[Appliances types] Whether or not a NILM approach will be successful crucially depends on which appliances are to be monitored. NILM as opposed to ILM solutions will oftentimes struggle with appliances that have highly varying signatures and appliances that consume little power. There are numerous appliances that NILM algorithms struggle with such as personal computers, printers, phone chargers, etc. However, for other appliances a NILM solution might yield appliance state estimates that are sufficient for the load monitoring application at hand such as appliances with regular signatures like e.g. traditional fridges and appliances with a high power draw such as kettles. Note that when multiple appliances of the same make and model are present within the aggregate, NILM algorithms will most likely fail to distinguish them. Because their features will most likely be close to identical, most NILM algorithms will lump these appliances together which in turn means that the value that NILM solutions provide is limited in such a scenario.
\item[Number of appliances of interest] If the specific load monitoring application only requires appliance energy estimates for very few appliances, there is little to gain from a NILM solution. However, when a more holistic energy monitoring approach is to be employed, i.e. when multiple load monitoring applications are to be tackled in parallel and the number of appliances for which load estimates are required is high, the potential savings by making use of NILM are high.
\item[Potential value] If the value of the load monitoring application is below a certain threshold, there is no incentive to either use an ILM or NILM solution. On the flip side, if the potential value of the load monitoring solution is above a certain threshold, it is hard to justify a non-intrusive solution. The additional cost of installing more meters outweigh the uncertainty that is inherent to a non-intrusive solution. NILM solutions will be the most economical approach to load monitoring between these two thresholds, i.e. when the potential value of the load monitoring application is large enough to warrant the installation of at least one meter and when potential value is small enough such that ILM is not economical.
\end{description}
\fi



\subsection{State of current research}

\subsubsection{Event-Based approaches}
\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{nilm_hart.png}
\caption[A graphical depiction of the NILM process.]{A graphical depiction of the NILM process taken from the seminal paper by Hart 1992. Taken from~\cite{hart1992nonintrusive}}
\label{fig:hart}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{nilm_event.png}
    \caption[Schematic of the data flow of generic event-based NILM algorithms.]{Schematic of the data flow of generic event-based NILM algorithms. Taken from~\cite{anderson2012event}.}
    \label{fig:hart2}
\end{figure}


 Figures \ref{fig:hart} and \ref{fig:hart2} show graphically how early approaches operated, i.e. they tackled the problem by detecting sudden changes~\cite{anderson2012event}, so called events, in the aggregate power followed by a feature extraction phase~\cite{patri2014extracting}, i.e. signatures of the detected events are extracted in the hopes that classification algorithms can associate appliances with the extracted event features~\cite{pereira2015semi}. After a sequence of labeled events is extracted, this discrete time series is then in turn transformed into a power trace for each appliance.\\
Even though this approach was refined in numerous ways by e.g. using transient and high frequency information as features for classification~\cite{norford1996non}, improving the energy estimation phase~\cite{giri2015energy} or post processing of the extracted event sequence~\cite{he2016non}, generalizable and accurate power estimates could not be achieved. One of the reason why these event-based approaches struggle is the fact that the individual algorithmic stages make independent and local decisions which leads to errors propagating through the different algorithmic steps, i.e. the classification stage could result in a nonsensical sequence of appliance switches, i.e. it could e.g. result in a sequence where an appliance is assumed of having turned \emph{off} at time point $t$ even though it already was assumed to be \emph{off} at time point $t-1$. The energy estimation stage of the algorithmic chain is then tasked to achieve something impossible, i.e. transform a nonsensical event sequence into power estimates of individual appliances.
\subsubsection{Temporal Motif Mining}
One approach to overcome the problem of errors propagating through the stages of event-based approaches is Temporal Motif Mining (TMM)~\cite{shao2012temporal}. Specifically, the approach tries to tackle potentially nonsensical event sequences. TMM in a sense, merges the classification and energy estimation phase, i.e. it tries to solve both problems jointly, specifically, by trying to match events such that 
\begin{enumerate}
\item The sum of power changes of an event sequence is nearly 0.
\item Every prefix sum of event sequences is positive.
\item Power changes cannot be smaller than a certain percentage of the biggest transition in an episode.
\end{enumerate}
However, since the learning objective of TMM is combinatorial, simplifying assumptions over possible event sequences were made, such that no more than $x$ events can lie between events from the same appliance. Even though these approaches avoided the problem of nonsensical event sequences successfully, the assumption that an appliance episode is zerosum is often overly simple because appliances often exhibit transient behaviors and furthermore, the approach suffers from the reliance on a perfect event detector, i.e. if an event was missed, TMM struggles. Thus, even though the problem of error propagation from the classification into the energy estimation phase is mitigated by TMM, errors can still flow from the event detection into the energy classification stage.

\subsubsection{State-based approaches}
In order to overcome the problem of error propagation through the algorithmic stages, ideally, energy disaggregation is performed in an end-to-end manner, i.e. by a single unified model that tells the generative story of the aggregate power measured at the main distribution. Because the states of several appliances evolve independently in parallel and the aggregate observation is a function of all hidden states, Factorial Hidden Markov Models lend themselves as a modeling choice~\cite{ghahramani1996factorial}. Factorial Hidden Markov Models are a generalization of Hidden Markov Models where multiple hidden chains evolve marginally independent and the aggregate observation $x_t \in \mathbb{R}^S$ at time point $t$, is a function of all hidden states $z_t \in \{0,1, ..., K\}^N$, with $S$ being the observation dimensionality, $K$ being the number of states each hidden chain can take and $N$ being the number of latent chains. See Figure \ref{fig:fhmm} for a representation of the associated graphical model. The joint distribution is defined as the following:
\begin{align}
p(x_{1:T},z_{1:T}) = \prod_t^T p(x_t|z_t)\prod_i^N p(z_{t,i}|z_{t-1,i})p(z_{0,i})
\end{align}

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{FHMM.png}
\caption[The graphical model corresponding to Factorial Hidden Markov Models.]{The graphical model corresponding to Factorial Hidden Markov Models. Multiple hidden chains evolve independently in parallel whereas the observation at time point $t$, $x_t$ is a function of all hidden. Graphic taken from~\cite{parson2012non}}
\label{fig:fhmm}
\end{figure}

Each appliance is then modeled by a single HMM chain and the aggregate observation constitute the measurements collected at the main electrical feed of the building. Supervised energy disaggregation can then be posed as inference in the associated probabilistic model, i.e. inferring $z^* = \arg\max_z p(z_{1:T}|x_{1:T})$, whereas unsupervised energy disaggregation can be posed as the learning problem. Note that inference is computationally intractable mainly because of the fact that the individual latent chains become dependent conditioned on the observation and fact that the number of possible states grows exponentially with the number of hidden chains. This also entails that unsupervised energy disaggregation is computationally intractable, because inference is usually required for learning.

\begin{description}
\item[Iterative Viterbi] The problem of computational intractability of learning and inference in FHMMs has been tackled in a number of ways. In~\cite{parson2012non}, the problem is tackled by iteratively applying the Viterbi algorithm to a single latent chain. Specifically, parameterized general models of appliance types were specified and then iteratively, the parameters were fit to the observation and the Viterbi algorithm was used to compute the most likely state sequence for an appliance type. Then, this appliances contribution to the aggregate was removed and the process was repeated. Even though, this procedure allows for inference as well as limited learning, it is not invariant to the order in which appliances are processed, i.e. processing appliances in different orders will lead to different results.
\item[Optimization] In comparison, the AFAMAP algorithm poses inference as an optimization, specifically an integer programming problem~\cite{kolter2012approximate,shaloudegi2016sdp}. Specifically, by introducing the one-at-a-time constraint postulating that only a single HMM chain can change states at any given point in time, posterior inference is converted into a convex quadratic programming problem by relaxing the integer constraints associated with the integer program that results from translating posterior inference into such a problem. Additionally, a robust mixture model is introduced that absorbs power associated with appliances for which no ground truth is provided and the difference signal is modeled to improve accuracy. Even though the inference engine seems powerful and could in principle be extended to perform exact inference of the mode of the posterior by e.g. employing branch-and-bound, this approach has drawbacks. The inference technique needs to be provided with ground truth and will only produce power estimates for appliances for which ground truth is provided. In the paper, the authors describe an unsupervised strategy for obtaining ground truth, however this approach seems to not generalize well to other datasets. On top of that, extending the algorithm to handle the learning problem is challenging since solving the convex quadratic program only yields the mode of the distribution, therefore making the application of EM-like algorithms difficult. Only hard-EM schemes like Viterbi learning could in principle be employed though they often underfit considerably~\cite{gutmann2008parameter}. Furthermore, a na\"ive implementation of such an EM-scheme would only improve the model parameters for the appliances for which ground truth estimates are available. Moreover, strategies that try to extract ground truth for all appliances by iteratively subtracting appliance power traces for which estimates are available struggle with subtraction artifacts, i.e. because appliance estimates are imperfect, subtraction will effectively result in the addition of residuals that depending on the amplitude can easily confuse the ground-truth extraction module of the algorithm.
\item[Markov Chain Monte Carlo] On the other hand, Markov Chain Monte Carlo (MCMC) techniques were employed in order to deal with the intractable posterior distributions of the FHMM distribution~\cite{johnson2013bayesian,jia2015fully,el2009rjmcmc,egarter2015paldi}. In~\cite{johnson2013bayesian}, Factorial Hidden Markov Models were additionally extended to handle semi-Markovian state transitions trying to overcome the implicit assumption introduced by the Markov property of Hidden Markov Models, namely that the distribution of state durations, i.e. the time each Markov chain spends in a single state, is geometric. In the context of energy disaggregation, assuming geometrically distributed state durations is usually not valid.\\
The general idea of MCMC algorithms is to construct a Markov Chain whose equilibrium distribution provides a sample of the intractable posterior distribution. It can be shown that Gibbs sampling~\cite{casella1992explaining}, i.e. repetitively sampling from the conditional posterior distribution, constitutes a Markov Chain whose equilibrium distribution can provide a sample from the posterior distribution and that the quality of the sample improves with the length of the Markov Chain~\cite{geman1987stochastic}. However, Gibbs-based MCMC techniques in the context of energy disaggregation have two drawbacks. First, because posterior distributions in the context of energy disaggregation exhibit strong multi-modality, Gibbs samplers often exhibit very slow mixing of the posterior and therefore require prohibitively long Markov chains in order to acquire a high-quality sample of the posterior. The multi-modality of the posterior distribution encountered in the context of energy disaggregation can be explained by the fact that multiple appliances often exhibit a similar power draw, thus more than one appliance might be able to explain away the aggregate consumption. For illustration, consider a scenario with 2 two-state appliances with comparable power draw and an aggregate observation $x'$ that is similar to the power consumption of each appliances. Thus we can assume that for the posterior the following holds:
\begin{align*}
p(z|x') &= \begin{pmatrix}
0&
0.5&
0.5&
0
\end{pmatrix}\\
 \text{with } z &=\begin{pmatrix} 0,0&
0,1 &
1,0 &
1,1 
\end{pmatrix}
\end{align*}

A Gibbs sampler that resamples a single latent variable will `get stuck' in either of the modes ($z = (0,1)$ or $z = (1,0)$), i.e. in this contrived example the modes constitute probability islands which the sampler will never escape. In more realistic scenarios, when all probabilities are strictly greater than 0, even though the sampler will escape modes eventually, this process can be prohibitively slow. Second, even though these approaches allow for learning by introducing distributions over model parameters, inference is computationally expensive. Because of the multi-modality of the posterior that causes slow mixing, for every data point, an often times quite long Markov chain needs to be computed. Ideally inference takes a functional form that incurs little computational costs.
\iffalse
\item[Variational Inference] As stated earlier, recent advances in Bayesian optimization, specifically, Variational Inference and in particular the introduction of recognition distributions have led to the development of methods that allow for amortized statistical inference. 
Amortizing VI has been employed to the problem of energy disaggregation~\cite{ng2016scaling}, however even though temporal dependencies were modeled by making use of Gaussian copulas, because feed forward neural networks were used to parameterize the auxiliary distribution $Q$, temporal dependencies could not be modeled at the edges of temporal minibatches. Another drawback of the approach is the choice of objective function in conjunction with the choice of the auxiliary distribution: as an objective the ELBO introduced in \eqref{eq:ELBO} was used. As stated earlier, implicitly, the ELBO objective minimizes the backward KL-divergence. It can be shown that minimizing the backward KL-divergence is zero-forcing, i.e. most probability mass in $Q$ will be assigned to the mode of $P$. Because as an auxiliary distribution a factored multi-variate Bernoulli distribution is chosen, i.e.
\begin{align*}
 q_\sigma(z) = \prod_i^N \sigma_i^{z_i}(1-\sigma_i)^{1-z_i}
\end{align*}
with $\sigma \in [0,1]^N$ being the vector of coin-flip probabilities, it can be shown that:
\begin{align*}
 \arg \min_{\sigma} D_{KL}(Q_\sigma||P) &= \arg \min_{\sigma} \sum_{z \in \mathcal{Z}} q_\sigma(z)\log \frac{q_\sigma(z)}{p(z)} = \arg \max_z p(z|x)
\end{align*}
Thus, minimizing the backward KL-divergence will lead to $Q$ learning the mode of $P$ which in turn entails that the resulting learning algorithm will assign most probability mass to a single latent configuration $z$. In the experience of the author, this is problematic in the context of energy disaggregation and leads to considerable underfitting. This is because, ideally, when a change in aggregate power suggest that an appliance has changed its state, the uncertainty of previous state changes should be taken into consideration. For example, consider a scenario where the algorithm is unsure whether an \emph{off}-transition should be associated with appliance $a$ or $b$ and future observations strongly indicate an \emph{on}-transition for appliance $a$, then the algorithm should assign more probability mass to the state sequence in which appliance $a$ turns \emph{off} in the past.
\fi
\item[Function Learning] Recently, approaches emerged that directly try to learn a function that maps aggregate observations to single appliance power traces or energy estimates~\cite{kelly2015neural}. These approaches usually assume knowledge of single appliance episodes and training is facilitated by creating synthetic aggregate observations by randomly summing up these episodes. Function approximators are then trained to infer the power or energy consumption of single appliances. These approaches make the implicit assumption that patterns introduced by other appliances when inferring the power consumption of one appliance can be considered noise and will be ignored by the function approximator. However, that assumption is in the opinion of the author not valid because the patterns introduced by other appliances are highly structured and will often confuse function approximators. Furthermore, apart from being inherently supervised, these approaches struggle with shifts in the data distribution, thus making even a simple shift in the baseload potentially a problem for these approaches. However, these approaches might find application in niche scenarios where the load composition is known and static. For other approaches based on this idea, see \cite{zhang2018sequence,barsim2018neural,salerno2018extreme,de2018appliance,roos1994using}
\end{description}

\subsection{Knowledge Gaps}

For state-based NILM algorithms, one of the biggest challenges seems to be the computational burden associated with the learning problem in graphical models appropriate for NILM. This problem seems to not have been solved sufficiently: some algorithms circumvent learning altogether by applying heuristics to acquire ground truth, other algorithms avoid the combinatorial nature of the problem by performing learning one-appliance-at-a-time whereas other approaches rely on computationally expensive and typically slow approaches for learning. Thus, existing algorithm either introduce considerable bias or are too expensive for real-world applications. Note that because exactly solving the learning problem is known to be a NP-hard problem, any successful algorithm performs some kind of approximation. Ideally, such an approximate algorithm is (asymptotically) unbiased and incorporates a parameter that allows to trade-off computational time for accuracy. An approximate but asymptotically unbiased algorithm that allows trading computational burden for accuracy is still missing.\\
Furthermore, so far, even though many publicly available data sets contain high frequency information, most algorithms typically only make use of information at a much lower sampling frequencies. Although event-based algorithms sometimes employ features extracted from high frequency measurements, for state-based algorithms, high sampling rates are oftentimes solely used to compute accurate active and reactive power measurements. Thus, state-based algorithms, so far, discard a potentially vital source of information and the question arises whether high frequency information can be used to improve state-based energy disaggregation algorithms.

\newpage
\subsection{Research Questions}
\subsubsection{Research Question 1.1}

Because state-based algorithms do not make sufficient use of high frequency information, the following research questions arise:

\begin{enumerate}
\item How can we leverage high frequency information from voltage and current measurements in a computationally efficient manner to obtain appliance state estimates?
\item What is the performance of the resulting algorithm in terms of disaggregation error in an unsupervised and supervised setting?
\end{enumerate}

The publication in section \ref{chapter:bolt} answers these research questions. The resulting paper was published in the \emph{Proceedings of the 3rd ACM International Conference on Systems for Energy-Efficient Built Environments}.

\newpage
\subsubsection{Research Question 1.2}
The algorithm resulting from answering research question 1.1 has shown promising results. However, it has some fundamental drawbacks. Because only micro-temporal dependencies are exploited, i.e. temporal dependencies within a single voltage-cycle, some degree of supervision is required to achieve acceptable disaggregation results. However, for a NILM algorithm to be commercially viable, disaggregating energy in an unsupervised fashion is paramount. When introducing temporal dependencies into the model, a computationally efficient strategy to approximate the forward probabilities defined in (\ref{eq:forward_probs_intro}) is required. Because of this, the following research questions arise:

\begin{enumerate}
\item How can a computationally efficient approximation of the filtering recursion be obtained that allows for temporal regularization?
\item What is the performance of the resulting algorithm in an unsupervised setting?
\end{enumerate}

The publication in section \ref{chapter:varbolt} answers the research questions above. The resulting paper was published in the \emph{Proceedings of the 2018 AAAI Conference on Artificial Intelligence}.

\newpage
\subsubsection{Research Question 1.3}

The algorithm introduced by answering Research Question 1.2 makes use of a factored multi-variate Bernoulli distribution. In order to avoid a parameterization that grows exponentially with the number of appliances, this distribution makes an unnecessary independence assumption, specifically, that states of appliances are independent given previous observations. As will be shown later, this independence assumption makes it difficult to learn either-or relationships. Because of this, the following question arises:

\begin{enumerate}
\item How can a distribution as flexible as a mutli-variate Bernoulli be parameterized such that the parameterization does not grow exponentially?
\end{enumerate}

This research question is answered in the publication in section \ref{chapter:factornet}. It was published in the \emph{Proceedings of the 4th International Workshop on Non-Intrusive Load Monitoring. 2018}.

\newpage
\subsubsection{Research Question 1.4}

The time-complexity of the algorithm introduced by answering Research Question 1.2 was mainly reduced by making an application-specific sparsity assumption as well as exploiting the structure of a factorized posterior distribution in conjunction with the factorization inherent to FHMMs. However, as stated earlier, using this factored posterior limits the accuracy of the inference technique and, on top of that, despite of the sparsity assumption, the scalability of the algorithm is limited. The question arises whether we can abolish these simplifying assumptions that were required to ensure computational efficiency but still be computational efficient. Note that because such an approach does not exploit structure unique to NILM, i.e. it is a generic algorithm for inference and learning in dynamical systems, the following question arises:

\begin{enumerate}
\item What is an asymptotically unbiased algorithm for inference in learning in non-linear stochastic dynamical systems with binary latent states?
\item How sample-efficient is the resulting algorithm?
\end{enumerate}

The publication in section \ref{chapter:nvif} answer this research question. It was published in the \emph{Proceedings of the 2019 International Conference on Acoustics, Speech, and Signal Processing}.

\newpage
\section{Problem Statement: AC Optimal Power Flow}


As stated earlier Alternating Current Optimal Power Flow (ACOPF) tries to answer the question of how to configure generators in an alternating current transmission network optimally such that generation meets demand. Generation meets demand when a set of non-linear constraints, the so called power flow equations, are satisfied. Optimality is defined by minimizing an objective function. ACOPF should not be confused with two similar and related problems, i.e. the load flow problem and economic dispatch. The former problem deals with inferring the system state given partial knowledge, i.e. solving the power flow equations, whereas the latter is concerned with optimally dispatching power whilst ensuring a power reserve but by simplifying or completely ignoring the power flow equations.

\subsection{State of current research}

Early approaches to optimally operate alternating current transmission networks relied on ``experienced engineers and operators using judgment, rules of thumb, and primitive tools"~\cite{cain2012history}. However, soon computational tools were introduced. In as early as 1929, analog network analyzers that model the transmission network were used to solve the power flow equations\cite{cain2012history}. Then, in 1956, Ward and Hale introduced the first automated and digital power flow solver~\cite{hale1956digital}. In 1962, Carpentier introduced the optimality conditions for the ACOPF problem based on Karush-Kuhn-Tucker conditions which is considered today to be the first formulation of the ACOPF problem~\cite{carpentier1962contribution}. Even though there are many different ACOPF formulations with different objectives, most ACOPF formulations can be represented by the following standard form:
\begin{align*}
min_x f(x)\\
\text{s.t. } g(x) &= 0\\
h(x) &\leq 0
\end{align*}
where $f$ is the objective function to minimize, usually the generation cost, and $h$ and $g$ are constraint functions that describe the power flow equations, system constraints and control limits. As stated earlier, in order to ensure that the system is in a physical state and that generation meets demand considering the transmission losses, the power flow equations need to be satisfied. There are multiple equivalent formulations of the power flow equations. However, the most commonly used are in polar form~\cite{schaffer1988nondiverging}:
\begin{align*}
0&=-P_{{i}}+\sum _{{k=1}}^{N}|V_{i}||V_{k}|(G_{{ik}}\cos \theta _{{ik}}+B_{{ik}}\sin \theta _{{ik}}) = g_i\\
0&=-Q_{{i}}+\sum _{{k=1}}^{N}|V_{i}||V_{k}|(G_{{ik}}\sin \theta _{{ik}}-B_{{ik}}\cos \theta _{{ik}}) = g_{i+N}
\end{align*}
Thus, the power flow equations describe the non-linear relationship between nodal voltages $V_i$ and active and reactive power ($P_i$ and $Q_i$) in the transmission network specified by the admittance matrix $Y_{ik} = G_{ik} + jB_{ik}$. Note that $N$ specifies the number of buses and $\theta _{{ik}}$ the difference in voltage angle between bus $i$ and $k$. Although equivalent, the power flow equations can also be expressed in matrix form with $S_i = P_i + jQ_i$, $S \in \mathbb{C}^N$, $V \in \mathbb{C}^N$ and $Y \in \mathbb{C}^{N \times N}$,
\begin{align*}
0 = S - diag(V)(YV)^* = g
\end{align*}

Note that in many instantiations of the ACOPF problem, the inequality constraints specify control limits of the nodal voltages and active and reactive power generation. Note that power generation constraints are often non-convex, i.e.
\begin{align*}
h_i = P_i - P^{max}_i o_i \leq 0\\
h_i =  P^{min}_i o_i - P_i \leq 0
\end{align*}
with $o_i \in \{0,1\}$ specifying whether a generator is online.\\
Even though the cost function $f$ is often well-behaved and e.g. convex, constraints are usually non-convex and non-linear.\\

Because the non-convexities encountered in the ACOPF constraint set can usually be represented by integer constraints, mixed integer program solvers based on e.g. branch-and-bound~\cite{lawler1966branch} can be employed to deal with the non-convexities. However, because multiple, and in the worst case exponentially-many, linearly relaxed problems need to be solved, these approaches incur substantial computational cost and require a \textbf{robust}\footnote{Robust in this context means that convergence to the physical solution can be guaranteed.} solver for the relaxed sub-problems.\\

Furthermore, the non-linearities encountered in the ACOPF problem pose additional challenges. Because of their non-linear nature, the power flow constraints do not exclude all non-physical solutions. It can be shown that for the much simpler load flow problem, a transmission system consisting of $N$ buses can have up to $2^N$ solutions even though there is only one single physical solution~\cite{thorp1997load,tamura1983relationship}. This entails that the load flow problem is underspecified, i.e. the power flow equations are solely a necessary condition for a solution to be physical but they are not sufficient.

\subsubsection{Decoupled-OPF}
In order to overcome the problem of under-specificity of the ACOPF formulation, approaches were developed that linearize the power flow equations~\cite{stott1974fast}. Specifically, by making two assumptions a bilinear approximation of the power flow equations can be obtained.
\begin{enumerate}
\item The imaginative part of the admittance matrix dominates the real part, thus assuming that $Y_{ik} = jB_{ik}$ is a valid approximation
\item Because voltage angles are small, $\cos(\theta_{ik}) \approx 1$ and $\sin(\theta_{ik}) \approx \theta_{ik}$
\end{enumerate}
which results in the equations:
\begin{align*}
0 = P_i - \sum_{k=1}^N |V_i||V_k| B_{ik} \theta_{ik} = g_i\\
0 = Q_i - \sum_{k=1}^N -|V_i||V_k| B_{ik} \theta_{ik} = g_{i+N}
\end{align*}
If furthermore it is assumed that voltage magnitudes are approximately unit, the power flow equations can be reduced to:
\begin{align*}
0 = P_i - \sum_{k=1}^N B_{ik} \theta_{ik} = g_i
\end{align*}
 Note that if the constraints are linear, introducing these simplifying assumptions results in a linear program that can be solved efficiently by e.g. the Simplex algorithm or Interior Point methods~\cite{klee1970good}.\\
Even though this formulation effectively disregards all network transmission losses, it is used in many commercial and industrial applications as a de facto standard but according to~\cite{stott2009dc}, the formulation introduces unacceptable errors in large systems.

\subsubsection{Full ACOPF solvers}
To avoid these errors for large systems, many different ACOPF solvers that operate directly on the non-linear power flow equations were proposed.

\begin{description}
\item[Sequential Linear Programming] One such algorithm to solve the ACOPF problem is Sequential Linear Programming (SLP)~\cite{kirschen1988mw}. SLP tries to solve non-linear programs by a series of linear approximations, i.e. SLP is an iterative procedure that solves a linear program at every iteration. Specifically, given a guess about the solution $x_0$, a linear program (LP) relaxation around $x_0$ is performed and the resulting LP is solved by an LP solver, typically either by variants of the Simplex Method or an Interior Point solver, in order to obtain a better guess $x_{i+1}$. The linear program relaxation is obtained by a first order Taylor series expansion and every iteration is proved to improve the objective, therefore guaranteed to find a local optimum.
\item[Sequential Quadratic Programming] Similarly to SLP, Sequential Quadratic Programming (SQP) is an iterative process that solves an optimization problem at every iteration~\cite{grudinin1998reactive}. However, instead of performing a first order Taylor expansion, a second order expansion is performed resulting in a quadratic program to solve at every iteration. Because second order information are leveraged, SQP usually converges faster than SLP. Note that when the program is unconstrained, SQL will reduce to performing Newton-Raphson.\\

These approaches implicitly assume that applicable functions display “suitable convexity”~\cite{carpentier1962contribution}, i.e. implicitly it is assumed that gradient-based approaches will find physical solutions despite the non-convexity of e.g. the power flow constraints. This becomes abundantly clear when realizing that all of the introduced solvers rely on an initial guess of the optimal solution $x_0$, also called the seed or initial point, and follow some kind of gradient that results from either solving an LP or QP or some program involving barrier or penalty functions. Ultimately, these solvers perform projected gradient steps. The authors of~\cite{hiskens2001exploring,schecter2013exploration} argue that given the structure of the power flow equations, the assumption of ``suitable convexity" may be a big assumption.\\

The fact that ``suitable convexity" might be an invalid assumption is further supported by research that investigates the convergence properties of solvers for the power flow equations. It can be shown that Newton-Raphson based solvers exhibit a property commonly referred to as \emph{Newton fractals}~\cite{klump2000new,thorp1997load}, i.e. convergence to the true solution can only be guaranteed if the seed point $x_0$ is ``close enough" to the true solution because ``the boundaries between basins of attraction under the iterative scheme are fractal. As two or more solutions get closer, their basins become more and more intertwined, so that, the neighborhood of any given solution becomes peppered by points attracted to different ones"~\cite{trias2012holomorphic}. Figure \ref{fig:fractals} shows an example of \emph{Newton fractals} graphically.\\

\begin{figure}
\includegraphics[width=\linewidth]{fractals.png}
\caption[Fractal basins of attraction for the two-bus load flow problem, under
the FDLF (Decoupled-PF) method.]{``Fractal basins of attraction for the two-bus load flow problem, under
the FDLF (Decoupled-PF) method. Initial seeds that lead to the correct solution are shown in
green; to the spurious solution in red; and non-convergence in black."~\cite{trias2012holomorphic}}
\label{fig:fractals}
\end{figure}

Because of \emph{Newton fractals}, these approaches are in danger of not finding any physical solution at all because as described earlier and shown in Figure \ref{fig:fractals}, the power flow equations have solutions which are not physical, i.e. there are assignments to the complex nodal voltages and complex power such that the power flow equations are fulfilled that can however not be realized physically. These spurious solutions act as attractors, i.e. the power flow solvers introduced earlier, because solutions are obtained by projected gradient steps, might be attracted to these false solutions, in order to reach their goal of satisfying the power flow equations.\\

Note that because these solvers are deterministic, the solution depends crucially on the guess of the initial solution $x_0$. That is why researchers have tried to alleviate these convergence issues by initializing $x_0$ in a way that encourages convergence to the true solution, i.e. by initializing $x_0$ with the solution to a simpler problem such as the solution to decoupled-OPF~\cite{leonidopoulos1995approximate,klump2000techniques,stott1971effective}.\\
\item[Continuation] Another strategy is to the employ the continuation method, in the context of ACOPF also called power stepping, by iteratively solving problems by scaling demand by a factor $\lambda$ and use the solution obtained for small $\lambda$ as the initial guess for a solution at a bigger $\lambda$ starting at $\lambda = 0$ until the original problem is recovered at $\lambda = 1$~\cite{milano2009continuous}. Especially for problems whose solution is close to the feasibility boundary where convergence issues are predominant, it is easier to obtain solutions further away from the feasibility boundary and then follow the solution paths to $\lambda = 1$. 
\item[Homotopy] A similar strategy is the Homotopy method whose general idea is the following: similarly to the continuation method a scaling factor $\lambda$ is introduced, however for the Homotopy method, $\lambda$ interpolates between two functions~\cite{okumura1991solution,ponrajah1989minimum}. Specifically, if one tries to infer complex voltages $v$, the Homotopic function $H$ can be defined as:
\begin{align*}
H(v,\lambda) = \lambda g(v) + (1-\lambda) \hat{g}(v)
\end{align*}
In this case $\hat{g}$ is some function and $g$ denotes the power flow equations. Varying $\lambda$ and following the solution path can allow for finding solutions for ill-conditioned problems, i.e. for problems that do not converge when the initial guess is a flat (all load voltage
magnitudes equal to 1 and all bus voltage angles equal to
0).\\

However, both the Continuation and Homotopy methods incur substantial computational cost because numerous ACOPF problems need to be solved for varying $\lambda$. Note that these costs multiply when integer constraints are handled by branch-and-bound algorithms. For every branch, a relaxed optimization problem needs to be solved and if this relaxed problem is in turn solved by the Continuation and Homotopy method which requires solving multiple problems for varying $\lambda$, the number of requires solutions oftentimes explodes.
\item[Holomorphic Embedded Load Flow Method] In the context of the load flow problem, these issues were recently addressed by the introduction of a solver called Holomorphic Embedded Load Flow Method (HELM) \cite{trias2012holomorphic,trias2015fundamentals}. Specifically, HELM overcomes the problem of multiple solutions by performing analytic continuation of a function at a solution known to be physical~\cite{beach2000reliable}. Note that analytic continuation yields a unique solution if the function to continue is holomorphic~\cite{range2013holomorphic}. Which solution is found depends on the point from which continuation is performed and the authors of HELM argue that holomorphic continuation from a known physical solution ``guarantees that the solution always corresponds to the correct operative solution, when it exists; and it signals the non-existence of the solution when the conditions are such that there is no solution"~\cite{trias2012holomorphic}. However, so far, HELM has only found applications in the realm of power flow as opposed to \emph{optimal} power flow.
\end{description}

\subsection{Knowledge Gaps}

As described earlier, because of the non-linearity of the power flow equations, existing optimal power flow solvers lack robustness because convergence to non-physical solutions that nevertheless satisfy the power flow equations cannot be ruled out. Furthermore, because of non-convex in particular integer constraints, these approaches incur substantial computational cost. To put it into the words of the FERC, ``even 50 years after the problem was first formulated, we still lack a \textbf{fast} and \textbf{robust} solution technique for the full ACOPF problem"~\cite{cain2012history}.\\
Furthermore, as stated earlier, HELM overcomes some of the convergence issues of traditional load flow solvers by embedding the power flow equations into holomorphic functions. Even though this strategy seems to alleviate the problem of the ambiguity of the power flow equations, there is no straight-forward way for HELM to be used in \emph{optimal} power flow applications.

\newpage
\subsection{Research Questions}
Existing approaches for ACOPF seem to predominately be based on optimization strategies and the question arises whether a learning-based approach can overcome some of the problems introduced earlier. Specifically, can a function be learned that produces optimal and feasible generator configurations given demand assignments reliably and fast? Note that because ACOPF solutions need to comply with numerous safety constraints, it is paramount that this function produces outputs that comply with these constraints. Furthermore note that, because generators can be shutdown completely, the action-space of such a learning based approach is non-convex. The following questions arise:

\begin{enumerate}
    \item How can the ACOPF problem be formulated as a learning problem?
    \item Given a learning-based formulation:
     \begin{itemize}
        \item How can a learning signal be obtained?
        %\item What is a load flow solver that allows for differentiation through its operators?
        \item How can safety constraints like e.g. voltage magnitude constraints be enforced?
        \item What is a computationally efficient strategy to deal with non-convex action spaces?
    \end{itemize}
\end{enumerate}



\chapter{Variational Inference}
Successful solutions to the problems discussed above have one thing in common: they allow to obtain an optimal binary vector. This process can be posed as statistical posterior inference in distributions over binary configurations. However, because the normalizing constant of these distributions usually involves enumerating all binary states, posterior inference is oftentimes computationally intractable because the number of possible binary configurations grows exponentially with the dimensionality of this vector, therefore requiring approximate inference techniques. For example, in the case of ACOPF, every generator is associated with a binary variable and even moderately sized problems can have up to 50 generators. Thus, na\"ive inference on posterior distribution of that size would require evaluating $2^{50} \approx 10^{15}$ generator configurations.\\
There seem to be a dichotomy in approximate approaches to deal with intractable posterior distributions. On one hand, there are Markov Chain Monte Carlo techniques which are a collection of tools to draw samples from the desired posterior. Specifically, a Markov chain is introduced whose equilibrium distribution converges to a sample from the posterior~\cite{geman1987stochastic}. It is well known that the quality of the sample increases with the length of the Markov Chain, however, this process is often too slow when the probability surface is multi-modal, i.e. the Markov Chain can `get stuck' in modes of the posterior which leads to prohibitively slow mixing~\cite{geyer1992practical}. 

On the other hand, Variational Inference~\cite{wainwright2008graphical} techniques have emerged as an alternative tool to deal with intractable posterior distributions. Variational Inference turns stastistical inference into an optimization problem by optimizing parameters of an auxiliary distribution $Q$ such that $Q$ best approximates $P$. Because the true posterior is approximated by an auxiliary distribution, unless $Q=P$, VI is an approximate inference technique, whereas MCMC is asymptotically exact albeit being slow. Recent advances have greatly improved the scalability, applicability, speed and accuracy of VI based approaches. Because of these advances, especially in terms of speed, modern VI approaches seem to be a prime candidate to control and observe physical systems in real time.

In this section a brief overview on Variational Inference is provided. For a more thorough discussion of VI the reader is refered to~\cite{blei2017variational,zhang2017advances}. The main idea behind VI is the following: Given a distribution $P$ for which posterior inference is intractable, posterior inference is translated into an optimization problem. Specifically, a tractable distribution $Q_\psi$ parameterized by the variational parameters $\psi$ is introduced and the parameters $\psi$ are optimized in such a way that $Q_\psi$ best approximates $P$ as measured by some divergence criterion. Then, in order to perform inference on $P$, because $Q_\psi$ is maximally similar to $P$ but tractable, inference is performed on $Q_\psi$ instead.\\
Because acquiring samples from or computing $p(z|x)$ is intractable, directly minimizing $D(q(z|x)||p(z|x))$ with $D$ being some divergence measure is usually intractable. In order to overcome this problem, a surrogate loss is minimized. By making use of Jensen's inequality~\cite{kuczma2009introduction}, it can be shown that:
\begin{align}
\log p(x) &= \sum_z \log p(x|z) p(z)\\
              &= \sum_z \frac{q(z|x)}{q(z|x)} \log p(x|z) p(z)\\
              &\geq D_{KL} [q(z|x) || p(z)] + \mathbb{E}_{q(z|x)} [\log p(x|z)]\label{eq:ELBO}\\
              & = \log p(x) - D_{KL}(q(z|x) || p(z|x)) \label{eq:lowerbound}
\end{align}
with $D_{KL}$ being the Kullback-Leibner divergence (KL-divergence), i.e.
\begin{align*}
D_{KL}(q(z|x) || p(z|x)) = \sum_z q(z|x) \log \frac{q(z|x)}{p(z|x)}
\end{align*}
For learning, i.e. when some parameters of $P$ are free, maximizing equation \ref{eq:ELBO} is equivalent to maximizing a lower bound of the data log-evidence $\log p(x)$. Hence, equation (\ref{eq:ELBO}) is called Evidence Lower Bound (ELBO). Specifically, one can see that when inspecting equation equation \ref{eq:lowerbound}, the bound is tight when $q(z|x) = p(z|x)$. Thus maximizing the ELBO defined in equation \ref{eq:ELBO} results in jointly maximizing the data log-likelihood as well as minimizing the backward KL divergence. Note that maximizing equation \ref{eq:ELBO} does not require knowledge of the true posterior $p(z|x)$. This is paramount because, in the cases we are interested in, the true posterior $p(z|x)$ is computationally intractable.
\section{Mean Field}
Different choices of the auxiliary distribution $Q$ lead to different instantiations of VI. One prominent instantiation is Mean Field approximation where $Q_\psi$ is assumed to be fully factored~\cite{ghahramani1996factorial,jaakkola1998improving}. However, Mean Field approximation has drawbacks: Ultimately, Mean Field results in a recursive learning objective that is similarly susceptible to local optima resulting from the multi-modality of the true posterior in the same way as MCMC techniques. Furthermore, because the auxiliary distribution is not a conditional distribution, i.e. the auxiliary distribution specifies $q_\psi(z)$ instead of $q_\psi(z|x)$ and the dependency on $x$ is introduced by minimizing the divergence to $p(z|x)$, inference cannot be amortized easily, i.e. the relationship between $x$ and $p(z|x)$ is not learned explicitly and therefore inference once new data is collected, requires running a rather expensive learning algorithm again.
\section{Speed: Amortizing VI}
Modern VI approaches amortize inference through the use of recognition distributions by parameterizing $q_\psi(z|x)$ with a neural network~\cite{kingma2013auto,mnih2014neural}. Specifically, $q_\psi(z|x)$ is assumed to be some function $f$ that takes $x$ as input, i.e. $q_\psi(z|x) = f^z_\psi(x)$ and because of recent successes of neural networks for non-linear optimization, $f^z_\psi$ is often chosen to be a neural network which in turn entails that the variational parameters constitute its weights. If this is the case, because the neural network links $x$ and $p(z|x)$, after training, posterior inference is as simple as a forward pass through a neural network and can therefore be carried with a much lower computational complexity. Such a neural network is often called `recognition network' and the associated auxiliary distribution is then called `recognition distribution'~\cite{kingma2013auto}. Thus, in such a setting, VI teaches a neural network to perform posterior inference and because evaluating a neural network is usually computationally cheap, posterior inference compared to other approaches is extremely fast.
\section{Applicability: Black Box VI}
Evaluating the ELBO introduced in equation (\ref{eq:ELBO}) poses challenges: For most distributions that model physical systems, because the auxiliary and true distribution do not form conjugate pairs, the expectation in equation (\ref{eq:ELBO}) does not have a closed form solution. This complicates computing the gradient of equation (\ref{eq:ELBO}) with respect to $\psi$ which in turn limits the applicability of Variational Inference techniques considerably. This problem has recently been alleviated by the introduction of Black-Box VI~\cite{ranganath2014black}. To paraphrase the main idea of Black-Box VI: In order to circumvent having to compute the exact gradient of the expectation with respect to $\psi$, an unbiased estimate of the gradient is computed by sampling from the auxiliary distribution. The introduction of Black-Box VI results in a generic framework that allows using, in principle, any auxiliary distribution to perform inference on any intractable posterior as long as both distributions share the same support. However, because the gradient is estimated by sampling from the auxiliary distribution, the auxiliary distribution is required to take a functional form that allows for efficient sampling. As we will in chapter \ref{chapter:factornet}, finding a parameterization of the auxiliary distribution that allows this is sometimes not trivial.
\section{Scalability: Stochastic VI}
The amount of data collected of energy systems, such as e.g. the data collected at the main distribution panel of a building or voltage phasors in a distribution network, has increased substantially over the recent years. This in turn poses additional challenges for inference techniques: they need to scale gracefully with the amount of available data points. This problem has recently been addressed by the introduction of Stochastic VI~\cite{mesbah2017stochastic}. Because the data log likelihood can be expressed as a sum over the entire data set, computing model updates scales unfavorably to large data sets, i.e. the computational cost associated with a single iteration grows with the size of the data set. The main idea of Stochastic VI to alleviate this problem is to randomly select mini-batches of the available data points and optimize the variational objective for this mini-batch similarly to how stochastic gradient descent performs gradient descent on randomly sampled points of the training set~\cite{zinkevich2010parallelized,bottou2010large}.
\section{Accuracy: Flows}
As stated earlier, in general, unless $Q$ can approximate $P$ perfectly, VI performs approximate inference, i.e. the KL-divergence between the auxiliary and true posterior does not reach 0. When choosing the functional form of the auxiliary distribution, an implicit assumption about the shape of the true posterior is introduced. Furthermore, because efficient sampling from the auxiliary posterior is required for Black-Box VI, the auxiliary distribution is often chosen from a family of fairly simple distributions such as multi-variate Gaussian for continuous latent variables~\cite{kingma2013auto}. However, such an assumption is overly simple in most cases resulting in unnecessarily high KL-divergence after training, i.e. the auxiliary distribution underfits to the true posterior. In order to overcome this problem for continuous latent variables, flows that reshape the auxiliary posterior distribution have been proposed~\cite{rezende2015variational,kingma2016improved}. The general idea is to apply parameterized, invertible and mass-preserving functions, also called flow-operators to the latent variable. Recently, flow operators with the universal approximator property have been proposed~\cite{huang2018neural,grathwohl2018ffjord} allowing for, in principle, arbitrary reshaping of the auxiliary distribution in order to achieve tighter fits to the true posterior and therefore improving the accuracy of VI approaches. Note that this area of research is still rapidly evolving.
\section{Intuition}
Modern VI approaches combine the ideas introduced earlier to ultimately arrive at an accurate, scalable, fast and general technique for posterior inference. Note that these advancement have been made fairly recently, i.e. the algorithmic tool chain that has become modern Variational Inference has only been at our disposal for a short amount of time and is still being developed rapidly. It might not be intuitive why and how such a modern VI approach speeds up inference and learning. The general intuition can be explained by means of analogy: Posterior inference is usually intractable because evaluating a sum in the case of discrete distributions or an integral in the case of continuous distributions is intractable. Thus, posterior inference is intractable because enumerating the latent space is intractable. However, the probability distributions of interest are often highly structured which in turn entails that most of the probability mass is located within smaller subregions of the latent space. Thus, instead of enumerating all of the latent space, it is usually sufficient to investigate the regions of high probability density. This however poses a challenge: How can we identify regions with high probability mass? Because sampling from the posterior is intractable, we cannot use the true posterior to identify high density regions. Instead, the auxiliary distribution is used to guide the `search for probability mass'. For learning, i.e. when parameters of $P$ are unknown, every iteration tries to lift the probability surface at those regions identified by the auxiliary distribution whilst at the same time improving the auxiliary distribution at identifying high probability regions. Then, after learning, the auxiliary distribution has become sufficiently good at identifying these regions and can therefore be used for inference.


\newpage
\chapter{BOLT: Binary Online Matrix Factorization}
\label{chapter:bolt}

As discussed earlier, the following publication tries to answer the question of how to incorporate high frequency information into state-based NILM algorithms and what a computationally efficient algorithm for inference of appliance states is. Furthermore, it poses the question how well an algorithm that solely makes use of single-cycle information can disaggregate energy in a supervised and unsupervised way.\\


Lange, Henning, and Mario Bergés. "BOLT: Energy disaggregation by online binary matrix factorization of current waveforms." \emph{Proceedings of the 3rd ACM International Conference on Systems for Energy-Efficient Built Environments.} ACM, 2016.

\input{bolt/paper.tex}

\section{Postamble}
This publication introduces an algorithm to obtain appliance state estimates solely based on information present within a single cycle of current. The algorithm approximates an otherwise NP-hard problem by estimating the gradient through the Heaviside non-linearity. The publication answers Research Question 1.1.1, because inference requires only a forward pass through a neural network and can be carried out in real-time on embedded hardware, therefore fulfilling the additional requirement of computational efficiency. Furthermore, Research Question 1.1.2 is answered by evaluating the performance in a supervised and unsupervised fashion. In a supervised setting, the algorithm shows competitive performance in terms of disaggregation error. However, note that the performance drops considerably in an unsupervised setting.\\
In order to achieve computational efficiency, any temporal regularization that is usually present in state-based NILM approaches is ignored. Because of this BOLT seems to overfit, i.e. the algorithm finds solutions that are not constrained enough. Specifically, when a single appliance changes its state, the state of multiple components switch. Note that enforcing temporal regularization is computationally expensive because it requires an approximation of the filtering recursion. In the next chapter, an algorithm is presented that makes use of Variational Inference to achieve this.

\chapter{VarBOLT: Approximate Learning in Factorial HMMs}
The following publication can be viewed as an extension of BOLT. Specifically, it tries to overcome the problem that BOLT requires supervision in order to achieve competitive disaggregation results. In order to achieve this, temporal dependencies between appliance states are modeled which in turn requires an approximation of the filtering recursion, i.e. the publication tries to answers the question what a computationally efficient algorithm to approximate the filtering distribution to ultimately achieve temporal regularization is. Furthermore, the performance of the resulting algorithm is evaluated in an unsupervised fashion.

Lange, Henning, and Mario Berges. "Variational BOLT: Approximate Learning in Factorial Hidden Markov Models with Application to Energy Disaggregation." \emph{Thirty-Second AAAI Conference on Artificial Intelligence. 2018.}

\label{chapter:varbolt}
\input{varbolt/paper.tex}

\newpage
\section{Postamble}

This publication answers Research Question 1.2.1 by introducing an algorithm we call VarBOLT that allows for an approximation of the filtering recursion in $\mathcal{O}(C^{\epsilon+1})$ where $\epsilon$ is the number of candidate latent states and $C$ is the number of inferred appliances. Note that this is mainly achieved by making a sparsity assumption and by exploiting the structure of a factored auxiliary distribution. Note that the sparsity assumption states that the number of appliances that are active at any given time is small which reduces the number of candidate latent states. However, note that this assumption limits the scalability of the resulting algorithm.

On top of that, as we will show in the next publication, the use of a factored auxiliary posterior distribution can limit the accuracy of the algorithm. Specifically, because of the structure of the auxiliary distribution that is required to achieve the computational speed-up, it is hard for the algorithm to learn either-or relationships. Furthermore, because inference is performed solely by consulting the auxiliary distribution, the algorithm does not allow to revise past decisions once new measurements have been collected.


\chapter{FactorNet: Multi-variate Bernoulli without indepdence assumptions}
\label{chapter:factornet}

The following publication investigates a crucial piece within Variational learning frameworks, namely the choice of the auxiliary distribution. Traditionally, in order to ease the computational burden, simple distributions that oftentimes lack flexibility are used. Specifically in the context of binary latent states, usually factored Bernoulli distributions are employed. As we will show in the publication, using such a factored Bernoulli distribution makes it difficult to learn either-or relationships between appliances. The questions the following publication tries to answer is if it is possible to introduce an auxiliary distribution that is as flexible as a non-factored multi-variate Bernoulli whilst at the same time avoiding a parameterization that grows exponentially like a na\"ive parameterization would.

Lange, Henning, and Mario Bergés. "FactorNet: Learning to Factorize Intractable and Multi-Modal Posterior Distributions for Energy Disaggregation." \emph{Proceedings of the 4th International Workshop on Non-Intrusive Load Monitoring. 2018.}
\input{factornet/paper.tex}

\newpage
\section{Postamble}
This publication answers Research Question 1.3 by showing that a parameterization that grows linearly as opposed to exponentially can be achieved by learning the chain-rule factorization of a multi-variate Bernoulli distribution. The algorithm we call FactorNet exploits the fact that a conditional multi-variate Bernoulli distribution is a uni-variate Bernoulli distribution. In the publication we show computationally efficient algorithms to obtain the joint as well as posterior probabilities. Because the joint distribution can be obtained, in principle, joint-constrative Variational Inference could be employed ~\cite{ambrogioni2018wasserstein}. Note that the auxiliary distribution that was introduced was evaluated on a toy problem in which no temporal dependencies were modeled. Making use of FactorNet in a scenario with temporal dependencies is not trivial because the filtering recursion would need to be approximated. The trick to ease the computational burden of approximating the filtering recursion that stemmed from Research Question 1.2 is not at our disposal anymore, because a non-factored posterior cannot be used to collapse the sum over all latent states of within the filtering recursion. The next publication will introduce an algorithm that makes use of FactorNet as its auxiliary posterior whilst at the same time being computationally efficient.

\chapter{NVIF: Neural Variational Identification and Filtering}
\label{chapter:nvif}

As stated earlier, VarBOLT eases the computational burden of approximating the filtering recursion by exploiting the structure of a factored Bernoulli and by making an application-specific sparsity assumption. Even though VarBOLT shows promising unsupervised disaggregation results, it has a number of drawbacks. First, the algorithm is biased because it exchanges the true posterior in the filtering recursion for the auxiliary posterior without correcting for the bias. Second, the auxiliary posterior is factored and has therefore limited capacity. Third, the sparsity assumption is application-specific and it limits the scalability of the algorithm. In the following publication the question is posed how an algorithm can be constructed that is asymptotically unbiased and that makes use of the non-factored auxiliary distribution FactorNet introduced in chapter \ref{chapter:factornet}. Note that the resulting algorithm is general in its nature and could in principle be applied to any non-linear stochastic dynamical system with binary latent states. Furthermore, the sample efficiency of the resulting algorithm is probed.


Lange, Henning, Mario Bergés and Zico Kolter. "Neural Variational Identification and Filtering for Stochastic Non-Linear Dynamical Systems with Application to Non-Intrusive Load Monitoring." \emph{Proceedings of the 44th International Conference on Acoustics, Speech, and Signal Processing. 2019.}

\input{nvif/paper.tex}

\section{Postamble}
This publication introduced an algorithm we call NVIF that can, in principle, perform inference and learning in any non-linear and stochastic dynamical system with binary latent states. It makes use of Importance Sampling and Monte Carlo Integration to achieve asymptotic unbiasedness, therefore answering research question 1.4.1. Note that NVIF allows to trade accuracy for computational time by choosing the number of samples to approximate the filtering recursion. Furthermore note that the algorithm becomes the Baum-Welch~\cite{baum1970maximization} algorithm when the entire latent space is explored. The resulting algorithm seems to exhibit promising sample efficiency: The data log-likelihood only increases marginally when increasing the number of samples from 400 to 500 which indicates that exploring roughly 1\% of the latent suffices to achieve an approximation similar to Baum-Welch. In chapter \ref{chapter:future_work} we explore possible future research paths of the proposed algorithm.


\newpage
\chapter{Towards Learning ACOPF}
There seems to be a dichotomy in approaches to control: learning and optimzation. However, most approaches to ACOPF are based on optimization strategies. Arguably, one of the reasons why optimzation based approaches dominate is the difficulty for learning based approaches to enforce security constraints that ACOPF solutions need to adhere to. In the following publication, the question will be answered how ACOPF can be reformulated as a learning problem. Furthermore, given a learning-based formulation, the question on how to deal with security constraint and non-convex action spaces is tackled. On top of that the question of how a learning signal can be obtained is answered. 
\label{chapter:lopf}
\input{krtofl/paper.tex}

\section{Postamble}
This publication introduced a learning based formulation of the ACOPF problem. It furthermore shows that a learning signal for the agent can be obtained by directly differentiating through a robust power flow solver and introduces computationally efficient strategies to deal with safety constraints and non-convex action spaces. Namely by learning an auxiliary function that produces a proxy of the KKT multipliers and by optimizing a Variational lower bound of the inverted cost function. The resulting algorithm is fast and robust, i.e. after training, the agent produces feasible load flow solutions that adhere to safety constraints in a timely manner. However, these load flow solutions are not optimal. In its current inception, the algorithm that we call LOPF could find applications as the initial seed point for traditional solvers or w but because






\iffalse
\section{Scaling HELM}
In the experience of the author, HELM struggles with big admittance matrices, i.e. it does not scale well with the number of buses. The author conjectures that this is the case because when the number of buses grows, the reduced admittance matrix $Y^r$ becomes `quasi-singular'. Note that as long as all buses are connected and there are no shunt elements in the network, the full admittance matrix $Y$ is singular because the sum over all rows is 0. By removing the slack row and column, i.e. obtaining $Y^r$, usually a matrix is obtained that is not singular. However for large number of buses, solving this matrix poses difficult, i.e. different solving techniques result in vastly different solutions. For example, solving the IEEE300 bus matrix ($Y^rx = 0$) with \emph{numpy}, \emph{scipy} and \emph{tensorflow} results in three different solutions, even though the respective error is miniscule, i.e. $||Y^rx - 0||$ is in the order $10^{-18}$ for all solutions.\\
The strategy to overcome this problem is the following: By changing the embedding, specifically by adding a $z$-dependent term that vanishes at $z=1$, the algorithm is freed of the burden of having to solve the reduced admittance matrix. Instead, for all computations, $Y^r$ is replaced by $(Y^r + (1-z)I\lambda)$. Note that the solution will not change, because for $z=1$, the original problem is recovered.

\begin{equation}
\sum_k (Y^r_{ik} + (1-z) \lambda I) \sum_n^{\infty} V_k[n]z^n = zS_i^*\sum_n^{\infty} W_i[n]z^n
\end{equation}

which can be rewritten as:

\begin{align}
\sum_k (Y^r_{ik} + (1-z) \lambda I) \sum_n^{\infty} V_k[n]z^n = zS_i^*\sum_n^{\infty} W_i[n]z^n\\
\sum_k (Y^r_{ik} + \lambda I) \sum_n^{\infty} V_k[n]z^n  - \lambda z\sum_n^{\infty} V_k[n]z^n = zS_i^*\sum_n^{\infty} W_i[n]z^n \\
\sum_k (Y^r_{ik} + \lambda I) \sum_n^{\infty} V_k[n]z^n  = S_i^*\sum_n^{\infty} W_i[n - 1]z^n + \lambda\sum_n^{\infty} V_k[n-1]z^n
\end{align}

Equating coefficients of the same order yields:

\begin{align}
\sum_k (Y^r_{ik} + \lambda I)  V_k[n] = S_i^*W_i[n - 1] + \lambda V_k[n-1] \label{eq:scale_embed}
\end{align}

Thus adding the $\lambda$-term in such a way that it does not change the solution at $z=1$ introduces a term dependent on the previous power series coefficient and removes the requirement of having to solve the reduced admittance matrix but a regularized admittance matrix instead. Since we iteratively solve for the power series coefficients starting at $n=0$, this only slightly changes the algorithm but might lead to stable solutions for big admittance matrices.
\fi


\chapter{Conclusion}

This thesis introduces applications of a modern approximate statistical inference technique called Variational Inference to engineering problems involving energy efficiency. The engineering problems constitute Non-Intrusive Load Monitoring (NILM) and Alternating Current Optimal Power Flow (ACOPF). Both problems share a common computational difficulty: inferring an optimal binary vector. However, the semantics of this optimal binary vector is different for both problem. For NILM, this vector constitutes the most likely state of appliances, whereas for ACOPF this vector describes which generators are entirely shut down. Because both problems share the same computational difficulty, the proposed solutions have a common core:

\begin{enumerate}
\item The computational problems associated with inferring an optimal binary vector are alleviated by making use of Variational Inference
\item An auxiliary distribution we call FactorNet whose parameterization grows linearly with the length of this optimal vector is employed
\item A decades old variance reduction technique, namely sampling without replacement, is employed that additionally avoids mode-collapse
\end{enumerate}

The combination of Variational Inference with FactorNet and sampling without replacement to some degree constitute the core contribution of this thesis. However, the bulk of contributions are within the realm of the domains of the respective applications.\\

In the case of ACOPF, this thesis furthermore shows that the robustness of ACOPF algorithms can be improved by differentiating through a load flow solver called Holomorphic Embedded Load Flow Method (HELM) and how, given a differentiable load flow solver, convex voltage magnitude constraints can be enforced within a Neural Network framework. Differentiating through a load flow solver and into a Neural Network greatly improves the speed of the resulting Reinforcement-Learning-like algorithm. The resulting algorithm is highly robust and fast but does not produce optimal generator assignments. The algorithm, in a sense, provides quick and dirty solutions to an otherwise computationally intensive problem and serves as an alternative to slow and brittle solvers. In the opinion of the author, the impact of the proposed solution on the power flow community will crucially depend on the underlying load flow solver HELM and whether or not the optimality gap can be closed in future research. However, the approach to enforce convex constraints can be adopted in any Neural Network architecture and might prove useful in other applications.\\

The application-specific contributions in the case of NILM are more general. Contributions within this thesis generalize Variational Inference to a class of graphical models with intractable joint distribution. This class encompasses dynamical systems where a binary latent state evolves over time. Note that the approach can be generalized to dynamical systems with continuous latent states by exchanging FactorNet with a continuous auxiliary distribution. If such a strategy is successful, the impact of the technology introduced in this thesis is potentially high: considering the technique could replace the non-optimal extended Kalman filters for non-linear state estimation. Note that non-linear state estimation algorithms have found widespread adoption in many fields apart such as e.g. building climate control, automotive applications, microgrids, networked control systems, operation research and finance, process control, robot and vehicle path planning, telecommunication network control and wind turbine control~\cite{mesbah2016stochastic}.

\chapter{Future Work}
\label{chapter:future_work}


\bibliographystyle{unsrt}
\bibliography{lit.bib}



\end{document}
